{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading library list...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import wrds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cvxpy as cp\n",
    "from arch import arch_model\n",
    "\n",
    "# Connect to WRDS\n",
    "db = wrds.Connection(wrds_username='simengut')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"\n",
      "\n",
      "Trying alternative approach with smaller chunks...\n",
      "Successfully retrieved data for SHY\n",
      "\n",
      "Successfully saved bond returns to CSV\n"
     ]
    }
   ],
   "source": [
    "# Define bond tickers\n",
    "bond_tickers = ['SHY']\n",
    "\n",
    "# SQL query to fetch bond data with a smaller date range and more efficient query\n",
    "bond_sql_query = \"\"\"\n",
    "SELECT \n",
    "    a.permno,\n",
    "    a.date,\n",
    "    b.ticker,\n",
    "    b.comnam,\n",
    "    b.cusip,\n",
    "    a.prc,\n",
    "    a.vol,\n",
    "    a.ret\n",
    "FROM \n",
    "    crsp.dsf AS a\n",
    "JOIN \n",
    "    crsp.dsenames AS b\n",
    "ON \n",
    "    a.permno = b.permno\n",
    "WHERE \n",
    "    b.ticker IN ('TLT', 'IEF', 'SHY', 'AGG', 'BND')\n",
    "    AND a.date BETWEEN '2007-01-01' AND '2023-12-31'  -- Reduced date range\n",
    "    AND a.date >= b.namedt\n",
    "    AND a.date <= b.nameendt\n",
    "ORDER BY \n",
    "    b.ticker,\n",
    "    a.date;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query with timeout and chunking\n",
    "try:\n",
    "    # Set a longer timeout and use chunking\n",
    "    bond_df = db.raw_sql(bond_sql_query, \n",
    "                        coerce_float=True,\n",
    "                        date_cols=['date'],\n",
    "                        chunksize=10000)  # Process in chunks of 10,000 rows\n",
    "    \n",
    "    # If the result is a generator (from chunksize), convert to DataFrame\n",
    "    if hasattr(bond_df, '__iter__'):\n",
    "        bond_df = pd.concat(bond_df)\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"First few rows of bond data:\")\n",
    "    print(bond_df.head())\n",
    "    \n",
    "    # Create a pivot table of returns\n",
    "    bond_returns = bond_df.pivot(index='date', columns='ticker', values='ret')\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nSummary statistics of bond returns:\")\n",
    "    print(bond_returns.describe())\n",
    "    \n",
    "    # Save to CSV\n",
    "    bond_returns.to_csv('bond_returns.csv')\n",
    "    \n",
    "    print(\"\\nBond ETFs included:\")\n",
    "    print(\"TLT: 20+ Year Treasury Bond ETF\")\n",
    "    print(\"IEF: 7-10 Year Treasury Bond ETF\")\n",
    "    print(\"SHY: 1-3 Year Treasury Bond ETF\")\n",
    "    print(\"AGG: US Aggregate Bond ETF\")\n",
    "    print(\"BND: Vanguard Total Bond Market ETF\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    \n",
    "    # Try alternative approach with smaller chunks\n",
    "    print(\"\\nTrying alternative approach with smaller chunks...\")\n",
    "    \n",
    "    # Process each ticker separately\n",
    "    all_dfs = []\n",
    "    for ticker in bond_tickers:\n",
    "        try:\n",
    "            ticker_query = f\"\"\"\n",
    "            SELECT \n",
    "                a.permno,\n",
    "                a.date,\n",
    "                b.ticker,\n",
    "                b.comnam,\n",
    "                b.cusip,\n",
    "                a.prc,\n",
    "                a.vol,\n",
    "                a.ret\n",
    "            FROM \n",
    "                crsp.dsf AS a\n",
    "            JOIN \n",
    "                crsp.dsenames AS b\n",
    "            ON \n",
    "                a.permno = b.permno\n",
    "            WHERE \n",
    "                b.ticker = '{ticker}'\n",
    "                AND a.date BETWEEN '2007-01-01' AND '2024-12-31'\n",
    "                AND a.date >= b.namedt\n",
    "                AND a.date <= b.nameendt\n",
    "            ORDER BY \n",
    "                a.date;\n",
    "            \"\"\"\n",
    "            \n",
    "            ticker_df = db.raw_sql(ticker_query, \n",
    "                                 coerce_float=True,\n",
    "                                 date_cols=['date'])\n",
    "            all_dfs.append(ticker_df)\n",
    "            print(f\"Successfully retrieved data for {ticker}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving data for {ticker}: {str(e)}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        # Combine all DataFrames\n",
    "        bond_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        # Create returns pivot table\n",
    "        bond_returns = bond_df.pivot(index='date', columns='ticker', values='ret')\n",
    "        \n",
    "        # Save to CSV\n",
    "        bond_returns.to_csv('bond_returns.csv')\n",
    "        \n",
    "        print(\"\\nSuccessfully saved bond returns to CSV\")\n",
    "    else:\n",
    "        print(\"No data was retrieved for any ticker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Can't reconnect until invalid transaction is rolled back.  Please rollback() fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)\n",
      "\n",
      "Trying alternative approach with smaller chunks...\n",
      "Error retrieving data for TLT: Can't reconnect until invalid transaction is rolled back.  Please rollback() fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)\n",
      "Error retrieving data for IEF: Can't reconnect until invalid transaction is rolled back.  Please rollback() fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)\n",
      "Error retrieving data for SHY: Can't reconnect until invalid transaction is rolled back.  Please rollback() fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)\n",
      "Error retrieving data for AGG: Can't reconnect until invalid transaction is rolled back.  Please rollback() fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)\n",
      "Error retrieving data for BND: Can't reconnect until invalid transaction is rolled back.  Please rollback() fully before proceeding (Background on this error at: https://sqlalche.me/e/20/8s2b)\n",
      "No data was retrieved for any ticker\n"
     ]
    }
   ],
   "source": [
    "# Define bond tickers\n",
    "bond_tickers = ['TLT', 'IEF', 'SHY', 'AGG', 'BND']\n",
    "\n",
    "# SQL query to fetch bond data with a smaller date range and more efficient query\n",
    "bond_sql_query = \"\"\"\n",
    "SELECT \n",
    "    a.permno,\n",
    "    a.date,\n",
    "    b.ticker,\n",
    "    b.comnam,\n",
    "    b.cusip,\n",
    "    a.prc,\n",
    "    a.vol,\n",
    "    a.ret\n",
    "FROM \n",
    "    crsp.dsf AS a\n",
    "JOIN \n",
    "    crsp.dsenames AS b\n",
    "ON \n",
    "    a.permno = b.permno\n",
    "WHERE \n",
    "    b.ticker IN ('TLT', 'IEF', 'SHY', 'AGG', 'BND')\n",
    "    AND a.date BETWEEN '2007-01-01' AND '2023-12-31'  -- Reduced date range\n",
    "    AND a.date >= b.namedt\n",
    "    AND a.date <= b.nameendt\n",
    "ORDER BY \n",
    "    b.ticker,\n",
    "    a.date;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query with timeout and chunking\n",
    "try:\n",
    "    # Set a longer timeout and use chunking\n",
    "    bond_df = db.raw_sql(bond_sql_query, \n",
    "                        coerce_float=True,\n",
    "                        date_cols=['date'],\n",
    "                        chunksize=10000)  # Process in chunks of 10,000 rows\n",
    "    \n",
    "    # If the result is a generator (from chunksize), convert to DataFrame\n",
    "    if hasattr(bond_df, '__iter__'):\n",
    "        bond_df = pd.concat(bond_df)\n",
    "    \n",
    "    # Display the first few rows\n",
    "    print(\"First few rows of bond data:\")\n",
    "    print(bond_df.head())\n",
    "    \n",
    "    # Create a pivot table of returns\n",
    "    bond_returns = bond_df.pivot(index='date', columns='ticker', values='ret')\n",
    "    \n",
    "    # Display summary statistics\n",
    "    print(\"\\nSummary statistics of bond returns:\")\n",
    "    print(bond_returns.describe())\n",
    "    \n",
    "    # Save to CSV\n",
    "    bond_returns.to_csv('bond_returns.csv')\n",
    "    \n",
    "    print(\"\\nBond ETFs included:\")\n",
    "    print(\"TLT: 20+ Year Treasury Bond ETF\")\n",
    "    print(\"IEF: 7-10 Year Treasury Bond ETF\")\n",
    "    print(\"SHY: 1-3 Year Treasury Bond ETF\")\n",
    "    print(\"AGG: US Aggregate Bond ETF\")\n",
    "    print(\"BND: Vanguard Total Bond Market ETF\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred: {str(e)}\")\n",
    "    \n",
    "    # Try alternative approach with smaller chunks\n",
    "    print(\"\\nTrying alternative approach with smaller chunks...\")\n",
    "    \n",
    "    # Process each ticker separately\n",
    "    all_dfs = []\n",
    "    for ticker in bond_tickers:\n",
    "        try:\n",
    "            ticker_query = f\"\"\"\n",
    "            SELECT \n",
    "                a.permno,\n",
    "                a.date,\n",
    "                b.ticker,\n",
    "                b.comnam,\n",
    "                b.cusip,\n",
    "                a.prc,\n",
    "                a.vol,\n",
    "                a.ret\n",
    "            FROM \n",
    "                crsp.dsf AS a\n",
    "            JOIN \n",
    "                crsp.dsenames AS b\n",
    "            ON \n",
    "                a.permno = b.permno\n",
    "            WHERE \n",
    "                b.ticker = '{ticker}'\n",
    "                AND a.date BETWEEN '2007-01-01' AND '2023-12-31'\n",
    "                AND a.date >= b.namedt\n",
    "                AND a.date <= b.nameendt\n",
    "            ORDER BY \n",
    "                a.date;\n",
    "            \"\"\"\n",
    "            \n",
    "            ticker_df = db.raw_sql(ticker_query, \n",
    "                                 coerce_float=True,\n",
    "                                 date_cols=['date'])\n",
    "            all_dfs.append(ticker_df)\n",
    "            print(f\"Successfully retrieved data for {ticker}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error retrieving data for {ticker}: {str(e)}\")\n",
    "    \n",
    "    if all_dfs:\n",
    "        # Combine all DataFrames\n",
    "        bond_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        \n",
    "        # Create returns pivot table\n",
    "        bond_returns = bond_df.pivot(index='date', columns='ticker', values='ret')\n",
    "        \n",
    "        # Save to CSV\n",
    "        bond_returns.to_csv('bond_returns.csv')\n",
    "        \n",
    "        print(\"\\nSuccessfully saved bond returns to CSV\")\n",
    "    else:\n",
    "        print(\"No data was retrieved for any ticker\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
